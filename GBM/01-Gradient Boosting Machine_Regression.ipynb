{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01-Gradient Boosting Machine / Regression.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPNokNmp6MBLT5ElXRx86B5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## **GBM (Regression)**"],"metadata":{"id":"Vvx60ZWuMdvz"}},{"cell_type":"code","source":["# 필요한 기본 package 불러오기\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","import matplotlib\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import r2_score\n","import time\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"_VFmp2HPMdIk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/KU-DIC/LG_time_series_day07.git"],"metadata":{"id":"gm3KrqxCM41r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **(1) 발전소 전기 에너지 출력(PE) 예측**"],"metadata":{"id":"ddn2eyl5Mt6j"}},{"cell_type":"code","source":["'''\n","CCPP Data : 6년 간(2006-2011) 발전소가 최대 부하로 작동할 때 시간당 순수 전기 에너지 출력(PE)를\n","평균 주변 온도(AT), 주변 압력(AP), 상대 습도(RH), 배기 진공(V) 센서 데이터를 이용해 예측하는 데이터\n","'''\n","# 데이터 불러오기\n","csv = pd.read_csv(\"/content/LG_time_series_day07/Data_GBM_CCPP.csv\", encoding = \"UTF-8-sig\")\n","csv.head(10)"],"metadata":{"id":"mmgH57VtMy5z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터 타입(type) 확인\n","csv.dtypes"],"metadata":{"id":"HMmXb3dLMdKT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습 데이터와 테스트 데이터 구분\n","\n","#학습 데이터 비율: 0.7, 테스트 데이터 비율: 0.3\n","train_data, test_data = train_test_split(csv, train_size = 0.7)\n","\n","#독립변수(Xs)와 종속변수(Y) 구분\n","train_X = train_data.iloc[:, :-1].reset_index(drop = True) # train_X에 종속변수 제거\n","train_Y = train_data.iloc[:, -1].reset_index(drop = True) # train_Y(종속변수) 따로 저장\n","\n","test_X = test_data.iloc[:, :-1].reset_index(drop = True) # test_X에 종속변수 제거\n","test_Y = test_data.iloc[:, -1].reset_index(drop = True) # test_Y(종속변수) 따로 저장"],"metadata":{"id":"aAE-X9SiMdL7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_X"],"metadata":{"id":"1vJCzZwHMdNr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# GBM 모델(예측) package 불러오기\n","from sklearn.ensemble import GradientBoostingRegressor\n","\n","#모델 파라미터 설정\n","'''\n","파라미터 목록: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\n","'''\n","model = GradientBoostingRegressor(loss          = \"ls\",\n","                                  learning_rate = 0.1,\n","                                  n_estimators  = 100,\n","                                  criterion     = \"mse\",\n","                                  max_depth     = 3,\n","                                  min_samples_split = 2,\n","                                  min_samples_leaf  = 1,\n","                                  verbose = 1)\n","\n","#설정된 모델 파라미터에 데이터 fitting (GBM 학습)\n","model.fit(train_X, train_Y)"],"metadata":{"id":"qtUlytpuMdPL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습한 GBM 모델을 통해 테스트 데이터 예측\n","\n","#pred 변수에 실제값 추가\n","pred = pd.DataFrame(test_Y)\n","\n","#생성된 모델로 예측하기 / pred 변수에 예측값 추가\n","pred[\"pred\"] = model.predict(test_X)\n","\n","# 예측 결과 확인 (첫 10 instances)\n","pred.head(10)"],"metadata":{"id":"EpfYJPfsMdQs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 예측 값과 실제 값 간의 오차 확인\n","\n","# Mean Absolute Error (MAE)\n","MAE = np.abs(pred.PE - pred.pred).mean()\n","print(\"MAE : \", MAE)\n","\n","# Mean Squared Error (MSE)\n","MSE = np.power(pred.PE - pred.pred, 2).mean()\n","print(\"MSE : \", MSE)\n","\n","# 오차 Plot\n","plt.figure(figsize = (5, 5))\n","plt.title(\"GBM\")\n","plt.scatter(pred.PE, pred.pred)\n","plt.xlabel(\"PE\")\n","plt.ylabel(\"pred\")"],"metadata":{"id":"OgINz2txMdSb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **(2) 터키 북서부 가스 터빈의 가스 방출량 예측**"],"metadata":{"id":"jM-78RXePEBg"}},{"cell_type":"code","source":["'''\n","pp_gas_emission dataset : 터키 북서부 지역에 위치한 가스 터빈에서 시간당 1시간동안 집계된 11개 센서, 36733개 instance가 5년치로 나누어져 있음 (날짜는 제공되지 않지만 시간순서로 정렬되어있음)\n","\n","타겟명은 'TEY'(Turbine energy yield, 터빈 에너지 생산량)이고, 주변 온도(AT), 주변 압력(AP), 주변 습도 (AH), 에어 필터 차압 (AFDP),\n","\n","가스 터빈 배기 압력 (GTEP), 터빈 입구 온도 (TIT), 터빈 후 온도 (TAT), 압축기 토출 압력 (CDP), 일산화탄소 (CO), 질소 산화물 (NOx)의 10개의 센서 데이터를 이용해 TEY 예측하는 데이터셋\n","'''\n","\n","# 데이터셋 불러오기 / 데이터 shape 확인\n","gt_df1 = pd.read_csv(\"/content/LG_time_series_day07/Data_GBM_gt_2011.csv\")\n","gt_df2 = pd.read_csv(\"/content/LG_time_series_day07/Data_GBM_gt_2012.csv\")\n","gt_df3 = pd.read_csv(\"/content/LG_time_series_day07/Data_GBM_gt_2013.csv\")\n","gt_df4 = pd.read_csv(\"/content/LG_time_series_day07/Data_GBM_gt_2014.csv\")\n","gt_df5 = pd.read_csv(\"/content/LG_time_series_day07/Data_GBM_gt_2015.csv\")\n","print('dataset shape:', gt_df1.shape)\n","print('dataset shape:', gt_df2.shape)\n","print('dataset shape:', gt_df3.shape)\n","print('dataset shape:', gt_df4.shape)\n","print('dataset shape:', gt_df5.shape)"],"metadata":{"id":"7w-QfhG4PA05"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 전체 데이터 위/아래로 병합 (concatenate)\n","\n","gt_df = pd.concat([gt_df1, gt_df2, gt_df3, gt_df4, gt_df5], axis=0)\n","gt_df.info()"],"metadata":{"id":"zClYCn2WPBeh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 독립변수(Xs)와 종속변수(Y, 'TEY') 구분\n","y_labels = gt_df.iloc[:, 7:8].reset_index(drop=True) # y_labels에 종속변수 저장\n","X_features = gt_df.drop(['TEY'], axis=1).reset_index(drop=True) # X_features에 독립변수 저장"],"metadata":{"id":"ac42BZ4XPBgP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습 데이터와 테스트 데이터 구분\n","# 학습 데이터 비율: 0.6, 테스트 데이터 비율: 0.4\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X_features, y_labels, test_size=0.4, random_state=0)"],"metadata":{"id":"ClQkASmuPBh_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.ensemble import GradientBoostingRegressor\n","\n","# GBM 모델 파라미터 설정 및 모델 생성\n","gb_reg = GradientBoostingRegressor(\n","    loss=\"ls\", n_estimators=500,  verbose=1, random_state=0)\n","\n","# 모델에 train dataset fitting (학습)\n","gb_reg.fit(X_train, y_train)"],"metadata":{"id":"nsMKFNpvPWP_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습한 GBM 모델을 통해 테스트 데이터 예측\n","gb_pred = gb_reg.predict(X_test)\n","\n","# 예측 값과 실제 값 간의 차이(오차) 계산\n","# Mean Absolute Error (MAE)\n","gb_mae = mean_absolute_error(y_test, gb_pred)\n","\n","# Mean Squared Error (MSE)\n","gb_mse = mean_squared_error(y_test, gb_pred)\n","\n","# Root Mean Squared Error (RMSE)\n","gb_rmse = (np.sqrt(mean_squared_error(y_test, gb_pred)))\n","\n","# 예측 값과 실제 값 간의 R^2(결정 계수) 계산\n","# 분산 기반으로 예측 성능을 평가 / 실제 값의 분산 대비 예측값의 분산 비율을 지표로 하며, 1에 가까울수록 예측 정확도가 높음\n","gb_r2 = r2_score(y_test, gb_pred)\n","\n","print(\"Testing performance\")\n","print('gb_MAE: {:.4f}'.format(gb_mae))\n","print('gb_MSE: {:.4f}'.format(gb_mse))\n","print('gb_RMSE: {:.4f}'.format(gb_rmse))\n","print('gb_R2: {:.4f}'.format(gb_r2))"],"metadata":{"id":"pDb73QYsPWdd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 500번의 반복(iteration)에 대해 예측 결과 할당\n","\n","# 빈 공간의 ndarray 생성\n","test_score = np.zeros((500,), dtype=np.float64)\n","\n","# 예측 값 (y_pred)와 실제 값 (y_test)의 loss를 test_score에 저장\n","# 학습이 진행될수록 loss값이 0에 가깝게 줄어드는 것을 확인할 수 있음\n","for i, y_pred in enumerate(gb_reg.staged_predict(X_test)):\n","    test_score[i] = gb_reg.loss_(y_test.TEY.values, y_pred)"],"metadata":{"id":"74cqjrKUPWfE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 500 iteration에 대한 학습 데이터 loss 값(파란선)과 테스트 데이터 loss 값(빨간선)을 확인할 수 있음\n","# 반복이 거읍될수록 loss값이 감소하며, 점차 0에 가까운 값으로 수렴함\n","\n","fig = plt.figure(figsize=(6, 6))\n","plt.subplot(1, 1, 1)\n","plt.title('Deviance')\n","plt.plot(np.arange(500) + 1, gb_reg.train_score_,\n","         'b-', label='Training Set Deviance')\n","plt.plot(np.arange(500) + 1, test_score, 'r-', label='Test Set Deviance')\n","plt.ylim(0, 2)\n","plt.legend(loc='upper right')\n","plt.xlabel('Boosting Iterations')\n","plt.ylabel('Deviance')\n","fig.tight_layout()\n","plt.show()"],"metadata":{"id":"sxCjFnKlPdl0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 의사결정나무 앙상블을 기반으로 하여, GBM 모델을 통해 변수 중요도 추출 가능\n","\n","# feature importance 산출 / feature importance 값이 높을수록 feature 중요도가 높다고 해석할 수 있음\n","feature_importance = gb_reg.feature_importances_\n","\n","# feature_importance를 높은 순서로 정렬\n","sorted_idx = np.argsort(feature_importance)\n","\n","# 변수 중요도 plot의 X축 길이를 선정하기 위한 설정\n","pos = np.arange(sorted_idx.shape[0]) + .5\n","\n","fig = plt.figure(figsize=(12, 6))\n","plt.barh(pos, feature_importance[sorted_idx], align='center')\n","plt.yticks(pos, np.array(X_features.columns)[sorted_idx])\n","plt.title('Feature Importance')\n","fig.tight_layout()\n","plt.show()"],"metadata":{"id":"1SPca1h7Pdns"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"JYOD8q2UPdpF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"MsTaIXuJPWgt"},"execution_count":null,"outputs":[]}]}